# 优化算法（Optimization Algorithms）
***
### batch 与 mini-batch
这一节中我们说的优化算法指的是可以提高训练速度的算法，而对于准确度先不予考虑。
那么什么是batch呢？其实之前我们一直在用的梯度下降方法就叫batch，想想我们是怎么做的。我们利用向量化的方法一次前向和后向传播对整个数据集做一次训练，然后进行一次参数修正，我们要训练1000次就要对整个数据集遍历1000次，假如数据集有10万条，那么我们的整个训练讲变得无比缓慢。怎么解决这个问题呢？就要引出mini-batch了。
其实它的思想也异常简单，不是嫌数据多吗，那我们就不用全部数据了，每次训练只用其中一部分，这就构成了mini-batch梯度下降法。我们根据输入的参数将数据集分为k份，每次训练拿其中的一份来跑梯度下降即可，常见的每份大小为：64、128、256等2的倍数。太大太小都不好。
那么这样做会产生什么变化呢？
就是损失函数不再单调下降了。
![](https://i.imgur.com/QizW066.png)
如这个图，因为我们每次处理的数据不一样，所以损失值也在震荡，但最终还是会下降到一个最优值的。

***
### 指数加权平均
指数加权平均是一个统计学中的概念，它的功能就是对震荡的离散数据进行平滑处理。
![](https://i.imgur.com/NWk5Z8K.png)
图中蓝点是一个地区半年内的气温数据，而红线就是应用指数加权平均求得的一个平均气温的走势图，可以看出还是相当契合的。那么怎么求呢？就是应用下面的公式
![](https://i.imgur.com/AdDJM2v.gif)
这里的θ就是原始数据点，而V就是平均化之后的数据点，β是一个超参数，一般取0.9效果可以。
那么这个公式的直观理解是什么呢？统计学告诉我们这个式子可以看成对前![](https://i.imgur.com/d9kLrKR.gif)个数据的平均，而且靠的越近的数据占的权值越高，有兴趣的朋友可以将上面的式子展开来看看是不是大概是这样的。
但是如果你展开来看的话就会发现一个问题，前几个数据似乎不太对，平均值好像太小了。确实会有这个问题，但因为这是一个统计学的式子所以其并不关心初始的几个值，如果对初始的几个数据比较敏感的话可以这样来修正一下：
![](https://i.imgur.com/ooSBOyX.gif)
这样当t很小时可以修正数据，而t变大时参数就趋近于1了。
这种方法对我们的梯度下降方法有什么用呢？别急，下面就来了。

***
### 动量梯度下降法（momentum gradient descent）

不管是batch还是mini-batch我们之前的梯度下降法中每次训练都是相对独立的，每次都是用一个完全新的dW和db来更新参数，这就会产生一些问题，如果我们的参数空间在几个维度上是不对称的，而我们的学习速度又过大的话，就会有这种情况：
![](https://i.imgur.com/kdISNti.png)
即震荡下降，这会极大的减慢我们的学习速率，甚至会出现梯度爆炸等情况。
怎么解决呢？就是用上面的指数加权平均的方法，我们让每次的参数改变不再独立，每次的值是如上面方法求得的平均值那么在其他方向的震荡会相互平均而抵消，只剩向着梯度方向的更新了，这样就会加快学习的速度，更快的到达最小点。
注意这里我们也不关注前几次的数据，所以可以不用修正项。
至于问什么这种方法叫动量梯度下降？如果我们将训练的过程看成在数据空间里的运动的话，那么每次的新dw、db可以看成一个加速度，而之前的可以看成速度，这样每次的运动就是由本次和之前的参数共同决定，相当于物理中的一个动量模型。

***
### RMSprop
这种方法和momentum很像，都是解决震荡问题，不过这种方法用的是向量模长的加权平均
![](https://i.imgur.com/Iislp5b.gif)
每次的参数更新变为：
![](https://i.imgur.com/UdFkG7M.gif)
参数b同理。
这类似与正态调整的思想，将向量除以它的模长，这样其在长的方向上会缩短，在短的方向上会变长，从而达到相同的目的

***
### 两者的结合之adam算法

adam其实没什么特别之处，它就是把momentum和RMS用到了一起。
需要说的就是其中的4个超参数：
α--学习速率-----需要根据实际确定
β1----momentun超参数 ------0.9比较合适
β2------RMS超参数 -------0.999比较合适
![](https://i.imgur.com/cUDOjmz.gif)----避免除0错的参数------1e-8比较合适

其他的就看代码吧

***
### 学习率衰减
如果我们之前的都是固定一个学习速率，这样最后可能会在最小值左右摆动而不收敛到位，如果这时候变小学习速率就有可能到达最优值，一种减小学习速率的方法是随着我们遍历整个数据集的次数来，譬如每处理完所有数据后就将学习速率减半，这是一种很有效的方法，当然还有一种简单粗暴的方法就是在训练一段时间后，人为的减小速率。

***
###局部最优问题

之前我们说如果学习速率设置的不好的话，可能会困在一个局部最优解，但是在一个很高维数据集的问题中是不用考虑的，因为在一个高维空间中，其在一点的每一维都是下凸的概率是非常小的，大概率的情况是一个马鞍点，这样：
![](https://i.imgur.com/IsT2Bzl.png)
所以不必担心其被困住的问题，唯一需要考虑的问题是，在马鞍点附近可能梯度变得很小，导致走的很慢，所以只要我们能承受时间上的计算量，一定可以走出去的！