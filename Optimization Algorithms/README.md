# 实用层面的一些技巧（Some tips in Deep Learning）
***
### 数据划分与交叉验证
在真实数据中，我们往往没有分类十分明确的数据集，往往是针对一个问题选则一个模型，根据一堆数据训练模型，而没有另外的数据来检测模型，这时候，我们就要对手中数据进行合理划分，从中分出训练集与测试集，一般的规模是80%的训练集和20%的数据集，采用随机划分的方法来进行分割，在python`sklearn.cross_validation`类中集成了数据的分割方法，可以直接调用。为了更充分的利用数据，还有一种方法叫“KFold”，这种方法是将数据分为K折，用其中的K-1折作为训练集，剩下的一折作为哦测试集，这种方法同样包含在上述的包内，而且是现在更常用的一种划分方法。

***
### 偏差与方差
偏差（bias）、方差（variance）是翻译过来的词，与我们平常理解的词义有所区别，高偏差指的是我们的模型欠拟合，对于训练集的准确度不高，高方差就是指我们的模型过拟合，对训练集的数据准确度过高从而出现过拟合情况，下面这个图形象的表示了这两种情况：
![](https://i.imgur.com/jGcs1cB.png)
如何区分高偏差与高方差这两种极端情况呢？最简单的就是通过我们模型在训练集和测试集中的错误率。
假如我们的训练集错误度为1%，而测试集为15%，则明显出现了过拟合情况，而如果两者都是15%左右则有可能是未完成训练，或参数不合适。
如果我们的的训练集为15%，测试集为30%则对应了高偏差的情况，也就是欠拟合。最理性的模型是二者都很低，但测试集稍高一些。
一些详细的技巧还需要根据实际问题具体分析，在实战中提高我们的技巧。

***
### 避免过拟合之正则化
先说说为什么正则化就可以避免过拟合吧，正则化简单来说就是给损失函数增加一项，这一项与参数的大小直接相关，那么我们在最小化损失函数的时候就必须同时最小化参数，参数小了有什么用呢？我们知道在神经网络中每个神经元的参数在一起构成一个大矩阵，而如果这个大矩阵中一些项变为了0，则对应原网络中去掉了一些神经元，神经元少了，模型简单了，自然会减少过拟合。正则化有两种思路，一种就是修改损失函数的表达式，这样在传播中不必更改内容就可以完成正则化，而另一种思路叫dropout正则化，就是在每一层中随机失活一些神经元来达到相同的目的，但这种方法更复杂些，下面就来仔细说说这两种正则方法。

***
### L1与L2正则化

我们原来的损失函数是这么定义的：
![](https://i.imgur.com/IPa7j4g.gif)
在后面加上一项叫做L2正则项式子，变为：
![](https://i.imgur.com/mgyEagU.gif)
其中：![](https://i.imgur.com/HgV7ufc.gif)
这里的lambda是一个超参数，而w的平方项实际上就是这个参数矩阵的大小，通过引入这一项我们就可以将参数矩阵与训练挂钩，试图找到一个比较小的最优参数矩阵。
同理如果后面加上的是L1正则项则变为了L1正则化，L1正则项为：
![](https://i.imgur.com/lOIF91p.gif)
即所有参数的绝对值之和。
这里需要说的是：在实际应用中L2比L1要更常用一些。
可以通过另外一个方面来再理解一下正则化：当参数矩阵w较小时我们输入激活函数的值就变小，而对于tanh这类的激活函数，当输入值在0附近时就变为了近似线性，这种线性就会导致这个网络退化，从而使得分类器变为线性分类，而在这种退化与过拟合之间存在的一种临近就是一种完美的最优解，因为是临近，所以寻找相对困难，需要耐心的调整超参数。

***
### droupout正则化
droupout正则化就是为每层的神经元设置一个失活概率，从而使得相应个数的神经元消失，这样做也可以相应的减少过拟合，但是也比上面的方法有更多的弊端，譬如：前向后向传播过程变复杂、损失函数不一定随训练次数下降等问题。所以这种方法不常用，了解就行。
这里再简单列一下其他避免过拟合方法吧：
- 扩增数据集
- 控制合理的训练次数
- so on

***
### 正则化输入

这个正则化不是用来避免过拟合的，而是用来加快训练速度和更好的找到最优解的。
因为我们的输入数据可能存在很多维，而这不同维之间数据的分布范围可能差别很大，譬如x1取值-3到3之间，而x2取值0.5-0.6之间，这就导致如果直接训练的话最后的损失函数会是一长条，如果不正好是向着最小值方向训练的话，我们可能是向着最低点摇摆前进，如下图：
![](https://i.imgur.com/B6ObwNq.png) 
这样可能导致我们步伐缓慢，甚至无法到达最低点，所以我们要用正太分布的思想，将数据全部正则化到0，1之间
![](https://i.imgur.com/XSCYhUa.gif)
μ是数据均值，σ是数据方差

***
###梯度消失与梯度爆炸

如果我们的网络足够深的话会发生什么情况呢？
现在假设所有神经元的w都一样，且b=0，那么前向传播的结果就是w的很多次方相乘，这时就会发生指数效应，假如说w比单位矩阵稍大一下，那么最后结果也会变得非常大，而如果w小于单位矩阵的话，那最后结果就接近0了，这两种情况均会导致我们无法取得合适的梯度，从而无法继续训练。为了避免这样的情况，我们就需要仔细考虑参数w的范围。
一种合理的操作为将每一层的w设置为和上一层的神经元个数有关，这样当输入神经元多时w就小一点，使得输出的结果尽量保持不变，反之亦然。
一般设为sqrt(1/n_l-1)。

***
### 梯度估计
上面我们所有求梯度的过程中均是精确计算的，不存在估计的问题，那么这里为什么要提估计呢？
是为了进行debug，也就是找错，如果一个复杂的传播过程最后结果是错的我们要找出哪里错了是不是比较麻烦？这时就需要对每一步的梯度传播用另一种方法求一下，看看是否相一致，如果不一致，那么我们很容易就可以找出在哪一层出现了错误，从而很方便的修改。
这就需要用到一点微积分的知识了，在微积分中导数的定义是函数值增长率的极限，即
![](https://i.imgur.com/9kT1ETP.gif)
这样，我们取θ为0.0001那么就可以近似得到导数，再进行对比，就可以发现bug了。