# 卷积神经网络（Convolution Neural Network）
***
### CNN能干啥？
在计算机视觉领域，处理的事情都是跟图片有关的，比如对图片内容进行分类，图片中的物体检测，识别等。回想一下我们之前做的猫咪分类器，好像也是对图片进行分类的，那我们直接用传统的神经网络不就行了？不是的，我们之前处理的图片大小是64*64*3的，这个大小相比其他的图片来说太小了，如果图片是1024*1024*3，那么如果应用传统NN的话，参数矩阵要多大呢？粗略算一下就知道要几亿，这对于内存和时间来说是无法接受的。所以我们要改进一下传统的NN来得到CNN。先说一下CNN为什么需要的参数小吧，它实际上是共享参数的，对于一个5*5的参数矩阵，图片的所有位置是共享的，而不像传统NN那样，同时它还是松耦合的，其每个神经元只与输入中的一小部分数据有关系，而对于其他数据它是不管的，这两点就可以将参数压缩到很小了。下面我们就来看看它是怎么操作吧。
***

### 卷积层

学过高数的人对卷积这两个字应该不陌生，在高数里，卷积指的是两个函数乘积的积分，十分恶心，在这里其实类似，但好理解很多。
![](https://i.imgur.com/em4Z32V.png)
上图中就是一个卷积运算，一个6*6的矩阵卷积一个3*3的矩阵，得到一个4*4的矩阵，其中的运算规则为：
结果左上角的第一个值等于左边大矩阵左上角3*3的部分与3*3的小矩阵元素依次相乘得到的，第二个答案是在大矩阵中右滑一步，从第二列开始的3*3矩阵相乘得到，其余答案同理，因为3*3的矩阵在6*6的矩阵中总共可以取16个位置，所以答案就是4*4的。推广一下，如果a*a的矩阵卷积b*b的矩阵，那么得到的答案是（a-b+1）*（a-b+1）的矩阵。这里的大矩阵就是图片的像素点构成的矩阵，而小矩阵就是参数矩阵，这样操作一次就叫做完成了一层传播。
***
### 边缘检测
再来看上面图片中的例子，它其实是一个边缘检测的例子，大矩阵代表的是每个像素的灰度，左半个是10代表亮的地方，右边是0代表暗的地方，这中间的边界通过那个特定的3*3矩阵就可以检测出来，结果就如上边图片所示。这个左边一列为1，中间一列为0，右边一列为-1的特殊矩阵就是计算机视觉中经常用的检测矩阵，将其转置后得到的矩阵同样可以用来进行水平边缘的检测。但注意，我们在CNN中不会用到任何这种现成的参数，所有的参数矩阵均是通过反向传播自己学得的，它的意义肯定没有这么明显。
***
### 填充操作（Padding）
如上面所说的，一个矩阵进行一次卷积运算后其大小会缩小一点，这对于很深的网络来说是十分不好的，我们不能让它很快的缩小到1，怎么解决的？就是运算前先往运算目标外面加几层，像这样。
![](https://i.imgur.com/HEiTxFs.png)
我们用一圈值为0的像素包围我们的矩阵，这个操作就叫padding，其参数p代表我们围了几圈，上边的公式改写如下：
a*a卷积b*b，等于（a-b+2p+1）*（a-b+2p+1）。通过这个操作就可以使矩阵大小保持。
***
### 步长参数（Strided）
上面例子中我们的小矩阵每次在大矩阵中移动一格来算一个答案，其实可以一次移动多格的，这又是一个新的参数s。这样我们的式子又复杂了，变为了[（a-b+2p/s)+1]*[（a-b+2p/s)+1]。
***

### 池化层与全连接层

这两个层与卷积层一起构成了整个CNN，它们起的作用更像是辅助，但它们也是必不可少的。池化层分为最大值池化和平均值池化，他们都非常好理解
![](https://i.imgur.com/n8ZO8Ew.png)
这是一个简单示意，对一个4*4的矩阵进行最大值池化，步长为2，作用即是对其中的每个2*2矩阵中元素求最大值，得到一个新的2*2矩阵。平均值池化即是求其中元素的平均值。
全连接层是啥？
其实就是传统的NN了，将矩阵中所有元素拆开，形成一个列向量，然后将其作为输入放进一个传统神经网络，这就是全连接层，一般在CNN的最后都是通过全连接层来得出最后结果的。
***
### 三维拓展

前面我们说的卷积都是对一个二维矩阵运算的，可是图片有RGB三通道，是一个三维矩阵啊，没关系，我们一般只对其一个通道感兴趣，其一个通道所带的信息已经足够我们用了，但如果我们把分类矩阵变了呢？以前我们是3*3的二维矩阵，要是我们有很多个3*3的矩阵呢？这当然可以,我们只需要将每个矩阵单独作用的结果叠起来行程一个有厚度的三维矩阵即可。
那么对于池化层呢？池化层只能对二维矩阵操作，如果输入的是一个三维矩阵，那么结果就是对每一维分别进行池化。

现在所有的概念都清楚了，让我们来看一个CNN的实例吧！

***
### 卷积神经网络示例
![](https://i.imgur.com/0ZSJmk5.png)
这是一个识别手写数字图片的CNN，类似与LeNet-5,它包含两个卷积层、两个池化层、两个全连接层以及最后的softmax层，各层的参数已经写在图中，注意有的文献将一次卷积与一次池化统称为一层，而有的文献称其为两层，这并不影响。