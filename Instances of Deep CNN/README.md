# 深层神经网络实例（Instances of Deep CNN）
***

###  经典卷积神经网络实例
这一节我们介绍几个经典的、已经在比赛或应用中取得好的结果的3个网络。分别是：LeNet-5、AlexNet、VGG-16。
其中LeNet是上世纪80年代的成果，而后两个则是近10年内的成果，后面明显可以看出这两个网络复杂得多。其中LeNet之前说过，是一个识别手写数字的网络，其输入是32×32×1的图片，注意它的输入是灰度像素，而不是传统的RGB三通道像素。其详细内容如下：
![](https://i.imgur.com/MWXx9O9.png)
总共两个卷积层、两个平均值池化、两个全连接层以及最后的一个softmax层，共包含6万个参数，是一个非常轻量级的网络。
AlexNet是2012年ImageNet大赛上获得冠军的作品，它的输入是一个227×227×3的RGB图片，详细内容如下：
![](https://i.imgur.com/FvmRS0e.png)
可以看出其网络还是非常漂亮的，大小在不断减小，深度在不断增大。这个网络总共有6000万个参数，是上一个的1000倍。
VGG-16是2015年新提出的网络，16指的是其总共有16个卷积层。还有一个VGG-19其实效果跟这个差不多，其具体的组成如下：
![](https://i.imgur.com/5ZeA9ac.png)
其最后是一个1000个点的softmax，可以对1000个分类进行预测。这个网络总共1300万个参数，跟AlexNet是一个级别的。

***
### 残差网络
传统的网络如果太深的话就会出现一些问题，比如有时候损失函数会随着深度先下降再上升，所以有时候传统网络在加深的时候就会出现一些问题，而这时一个新的网络种类就出现了，叫残差网络（Residual Networks）这个网络可以做到100层，甚至1000层的时候结果不会变差，它是怎么做到的呢？我们来看看。
在传统网络中，每一层的输入都严格的是上一层的输出，而残差网络恰恰不是，它让一层的输出跨过几层影响其他层。一个例子如下：
![](https://i.imgur.com/UJwVAo3.png)
![](https://i.imgur.com/8QM8Esz.gif)
关键就在最后那个式子，它让一层的输出跨过一层作为下一层激活单元的输入。这样的一个结构就成为一个残差块，而由很多残差块构成的网络就成为残差网络。
那么为什么这样的残差块可以使深层的网络表现更好呢？
一种解释如下：
为什么我们给一个网络加几层之后反而结果变差了呢？是因为当参数复杂之后网络学习一个恒等函数都十分困难了。那么我们就让它能够学恒等函数，回看上边的图和式子，我们发现只要我们中间的l+1层的W和b都是0，那么自然这个残差块就是一个恒等函数，这样至少我们将很多残差块叠加后能保持结果不变，在某些情况下结果会变好一点。这样网络越深，结果就会像越好的地方发展。

***
### 1×1卷积

1×1卷积是啥？那不就是把一个矩阵统一乘一个系数吗。没错，表面看是这样，但它也有它的作用。现在，假如说一层的输入是6×6×256，它大小很小，但深度很深，如果我们要保留其宽高信息而减少其深度该怎么办？1×1的卷积单元就可以满足这个功能，我们只需要32个1×1×256的探测器就可以将深度减为32，这种操作会为某些运算减少运算量，比如我们下边说到的一个例子。
***
### Inception网络

先来看1×1卷积是怎么减少计算量的。
一个如下的卷积层，将一个28×28×192的数据用5×5的卷机器做same卷积将深度变为32。
![](https://i.imgur.com/dCH53nm.png)
这个操作总共需要的计算量已经在图中写出为120M
如果我们在中间引入一个1×1的卷积层，并保持输入输出不变的话（如下图）那么计算量又变为多少了呢？12.4M，减少了讲近10倍。
![](https://i.imgur.com/11XxKgj.png)
这个减小计算量的小技巧通常用在计算量相比结果成本更高的一些网络中，可以显著提高速度。
那么什么是Inception网络呢？
它是Goole公司的一些人在2014年提出的一个CNN模型，它不再让人们决定网络中该用几乘几的卷积模块，而是自己在学习中决定用多大的合适。它的基本思路如下：
![](https://i.imgur.com/r9IWAZu.png)
它先对输入尝试每个大小的卷积模块，以及池化模块，然后将他们的输出拼接起来作为输出，注意这里的MAX-POOL操作也是要加入pad的，这样才能保证最后输出大小是一致的，这样在学习过程中它就会自己知道该偏向于选择一个什么大小的卷积，而为了减少计算量，它内部用了上面我们说过的1×1卷积，其详细的过程如下:
![](https://i.imgur.com/hQRcAwc.png)
这样的一步便称为一个Inception模块，而很多这个模块首尾相连便形成了整个Inception网络。
![](https://i.imgur.com/55nc3Cb.png)
非常有趣的是，其名字来源于盗梦空间中的一个名句“WE NEED TO GO DEEPER”，也算是最网络流行梗的一个巧妙运用吧2333333
***
### 利用开源数据进行迁移学习

在现在这个AI爆炸的时代，如果我们想利用学到的深度学习知识解决一个问题的话，一个很好的方法就是先看看互联网上有没有其他人进行过相关的探索，在Github上有很多人将自己搭建的网络开源出来，其中很多经过了很久训练的参数也可以一并下载，这给我们提供了很大的方便，我们不用从头开始自己的搭建工作，而可以根据自己手里的数据量和开源的网络来直接进行迁移学习，这样会节省很大的功夫。当然，如果你有兴趣的话完全可以尝试自己独特的构建一个网络，然后将其开源出来。